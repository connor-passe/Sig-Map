{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from tabulate import tabulate\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import shuffle\n",
    "from keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data into Training, Cross Validation, and Testing Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['boston', 'london', 'merthyr', 'nottingham', 'scarhill', 'southampton', 'stevenage']\n",
    " \n",
    "wavelengths = ['0.91595','0.449425','1.8025','2.695','3.6025','5.85']\n",
    "\n",
    "train_X = pd.DataFrame()\n",
    "cv_X = pd.DataFrame()\n",
    "test_X = pd.DataFrame()\n",
    "\n",
    "for city in cities:\n",
    "    for wavelength in wavelengths:\n",
    "        if (city == 'merthyr'):\n",
    "            df = pd.read_csv(f\"../Data/trainingDataset/{city}{wavelength}.csv\")\n",
    "            cv_X = pd.concat([cv_X, df])\n",
    "    \n",
    "        elif (city == 'stevenage'):\n",
    "            df = pd.read_csv(f\"../Data/trainingDataset/{city}{wavelength}.csv\")\n",
    "            test_X = pd.concat([test_X, df])\n",
    "\n",
    "        else:\n",
    "            df = pd.read_csv(f\"../Data/trainingDataset/{city}{wavelength}.csv\")\n",
    "            train_X = pd.concat([train_X, df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop N/A's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency                  0\n",
      "Power Loss               631\n",
      "Distance                 631\n",
      "Height Difference        631\n",
      "Peak Avg. Height Diff    631\n",
      "Peak Avg. Dist.          631\n",
      "Max Peak                 631\n",
      "Peak Count               631\n",
      "dtype: int64\n",
      "Frequency                0\n",
      "Power Loss               0\n",
      "Distance                 0\n",
      "Height Difference        0\n",
      "Peak Avg. Height Diff    0\n",
      "Peak Avg. Dist.          0\n",
      "Max Peak                 0\n",
      "Peak Count               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_X.isna().sum())\n",
    "train_X = train_X.dropna()\n",
    "cv_X = cv_X.dropna()\n",
    "test_X = test_X.dropna()\n",
    "print(train_X.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle datasets for future plotting purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = shuffle(train_X)\n",
    "cv_X = shuffle(cv_X)\n",
    "test_X = shuffle(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = train_X.pop(\"Power Loss\")\n",
    "cv_Y = cv_X.pop(\"Power Loss\")\n",
    "test_Y = test_X.pop(\"Power Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove non-needed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158668    28.0\n",
       "104905    18.0\n",
       "64288     29.0\n",
       "34134     54.0\n",
       "18206      1.0\n",
       "          ... \n",
       "120368    17.0\n",
       "26247     18.0\n",
       "215990    16.0\n",
       "12429     33.0\n",
       "10554     15.0\n",
       "Name: Peak Count, Length: 708053, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.pop('Peak Avg. Height Diff')\n",
    "train_X.pop('Peak Avg. Dist.')\n",
    "train_X.pop('Max Peak')\n",
    "train_X.pop('Peak Count')\n",
    "\n",
    "cv_X.pop('Peak Avg. Height Diff')\n",
    "cv_X.pop('Peak Avg. Dist.')\n",
    "cv_X.pop('Max Peak')\n",
    "cv_X.pop('Peak Count')\n",
    "\n",
    "test_X.pop('Peak Avg. Height Diff')\n",
    "test_X.pop('Peak Avg. Dist.')\n",
    "test_X.pop('Max Peak')\n",
    "test_X.pop('Peak Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Universal CallBack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 18:38:36.297251: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-10-29 18:38:36.297406: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-10-29 18:38:36.378355: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-10-29 18:38:36.460172: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-10-29 18:38:36.480758: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.5693133  12.714654  -73.43517  ]]\n"
     ]
    }
   ],
   "source": [
    "normalizerXL = tf.keras.layers.Normalization(axis=-1)\n",
    "\n",
    "normalizerXL.adapt(np.array(train_X))\n",
    "\n",
    "print(normalizerXL.mean.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionary for Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = {}\n",
    "\n",
    "cv_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grid_search(batch_size):\n",
    "    model = keras.Sequential([\n",
    "        normalizerXL,\n",
    "        layers.Dense(64, activation = 'relu'),\n",
    "        layers.Dense(64, activation = 'relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss = 'mean_absolute_error',\n",
    "        optimizer = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "    )\n",
    "\n",
    "    history_dnn = model.fit(\n",
    "        train_X,\n",
    "        train_Y,\n",
    "        epochs = 500,\n",
    "        batch_size = batch_size,\n",
    "        validation_data = (cv_X, cv_Y),\n",
    "        verbose = 1,\n",
    "        callbacks=[callback])\n",
    "\n",
    "    test_results[f\"Batch Size: {batch_size}\"] = model.evaluate(\n",
    "        test_X,\n",
    "        test_Y,\n",
    "        verbose = 1)\n",
    "\n",
    "    cv_results[f\"Batch Size: {batch_size}\"] = model.evaluate(\n",
    "        cv_X,\n",
    "        cv_Y,\n",
    "        verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Units Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_grid_search(num_of_units):\n",
    "    model = keras.Sequential([\n",
    "        normalizerXL,\n",
    "        layers.Dense(num_of_units, activation = 'relu'),\n",
    "        layers.Dense(num_of_units, activation = 'relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss = 'mean_absolute_error',\n",
    "        optimizer = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "    )\n",
    "\n",
    "    history_dnn = model.fit(\n",
    "        train_X,\n",
    "        train_Y,\n",
    "        epochs = 500,\n",
    "        batch_size = 100000,\n",
    "        validation_data = (cv_X, cv_Y),\n",
    "        verbose = 1,\n",
    "        callbacks=[callback])\n",
    "\n",
    "    test_results[f\"Units: {num_of_units}\"] = model.evaluate(\n",
    "        test_X,\n",
    "        test_Y,\n",
    "        verbose = 1)\n",
    "\n",
    "    cv_results[f\"Units: {num_of_units}\"] = model.evaluate(\n",
    "        cv_X,\n",
    "        cv_Y,\n",
    "        verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_sizes = [4, 16, 32, 128, 512, 2048, 8192, 32768, 131072]\n",
    "# for batches in batch_sizes:\n",
    "#    batch_grid_search(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Units Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_units = [2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "for units in num_of_units:\n",
    "     unit_grid_search(units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cv_results, index=['MAE [dB]']).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee7d7838ef53998fd22ad7449b76e48b4013ea11e59d28ee193f2cd757746339"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
